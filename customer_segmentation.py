# -*- coding: utf-8 -*-
"""Customer segmentation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OieQm-iQ93uZwxWsJr5IS0RTlfvMYkd6
"""

# Install the gdown library
!pip install gdown

# The Google Drive file ID is extracted from the URL
file_id = '11KyCBtciO18k4WUJ3Nh8SBwd8b7OlOqr'
output = 'customer_segmentation.csv'

# Download the file
import gdown
gdown.download(f'https://drive.google.com/uc?id={file_id}', output, quiet=False)

# Import necessary libraries
  import pandas as pd
  import matplotlib.pyplot as plt
  from sklearn.cluster import KMeans
  from sklearn.preprocessing import StandardScaler
  import seaborn as sns

# Load the dataset
data = pd.read_csv('customer_segmentation.csv')

# Display the first few rows to understand the data
print("Initial Data:")
print(data.head())
print("\nData Info:")
print(data.info())
print("\nData Description:")
print(data.describe())

# --- Data Preprocessing ---
# Select the features for clustering: Annual Income and Spending Score
# This helps simplify the project for educational purposes
X = data[['Annual Income (k$)', 'Spending Score (1-100)']]

# It's important to scale the data so that all features contribute equally to the clustering
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
print("\nScaled Data:")
print(X_scaled[:5])

# --- Finding the optimal number of clusters (K) using the Elbow Method ---
# The Elbow Method helps us choose the best 'k' for K-Means
wcss = []
for i in range(1, 11):
    kmeans = KMeans(n_clusters=i, init='k-means++', random_state=42, n_init=10)
    kmeans.fit(X_scaled)
    wcss.append(kmeans.inertia_) # Inertia is the sum of squared distances of samples to their closest cluster center.

# Plot the Elbow Method graph
plt.figure(figsize=(10, 6))
plt.plot(range(1, 11), wcss, marker='o', linestyle='--')
plt.title('Elbow Method for Optimal K')
plt.xlabel('Number of Clusters (K)')
plt.ylabel('Within-Cluster Sum of Squares (WCSS)')
plt.xticks(range(1, 11))
plt.grid(True)
plt.show()

# --- Apply K-Means with the optimal number of clusters ---
# Based on the plot, we will choose a value for K where the WCSS starts to decrease linearly
# Let's assume K=5 is the optimal number of clusters from the elbow plot
optimal_k = 5
kmeans = KMeans(n_clusters=optimal_k, init='k-means++', random_state=42, n_init=10)
data['Cluster'] = kmeans.fit_predict(X_scaled)

# Print the number of customers in each cluster
print("\nNumber of customers in each cluster:")
print(data['Cluster'].value_counts())

# --- Visualization of the Clusters ---
# Visualize the clusters using a scatter plot
plt.figure(figsize=(12, 8))
sns.scatterplot(x='Annual Income (k$)', y='Spending Score (1-100)', hue='Cluster', data=data, palette='viridis', s=100, style='Cluster')
plt.title(f'Customer Segments (K={optimal_k})')
plt.xlabel('Annual Income (k$)')
plt.ylabel('Spending Score (1-100)')
plt.legend(title='Cluster')
plt.grid(True)
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns

# --- Visualization of the Clusters ---
# Visualize the clusters using a scatter plot
plt.figure(figsize=(12, 8))
sns.scatterplot(x='Annual Income (k$)', y='Spending Score (1-100)', hue='Cluster', data = data, palette='viridis', s=100, style='Cluster')
plt.title(f'Customer Segments (K={optimal_k})')
plt.xlabel('Annual Income (k$)')
plt.ylabel('Spending Score (1-100)')
plt.legend(title='Cluster')
plt.grid(True)
plt.show()